<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="https://gobad820.github.io//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://gobad820.github.io//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://gobad820.github.io//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="https://gobad820.github.io//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://gobad820.github.io//apple-touch-icon.png">

<meta name="description" content="이곳에 설명을 입력하세요"/>

<title>
    
    Linear-Models(ML) | Pelikan&#39;s Glide
    
</title>

<link rel="canonical" href="https://gobad820.github.io/posts/linear-models_ml/"/>

<meta property="og:url" content="https://gobad820.github.io/posts/linear-models_ml/">
  <meta property="og:site_name" content="Pelikan&#39;s Glide">
  <meta property="og:title" content="Linear-Models(ML)">
  <meta property="og:description" content="이곳에 설명을 입력하세요">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-02T18:42:01+09:00">
    <meta property="article:modified_time" content="2025-04-02T18:42:01+09:00">
    <meta property="article:tag" content="ML">
    <meta property="article:tag" content="LinearModel">













<link rel="stylesheet" href="/assets/combined.min.97e32d65fe11c0cd3781001e6be3bf3516248c01a93ff6256f1e75541632eddd.css" media="all">
<link rel="stylesheet" href="/css/style.css">









  </head>

  

  
  
  

  <body class="auto">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">
        <a href="https://gobad820.github.io/">Pelikan&#39;s Glide</a>
    </h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/posts/">Posts</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/posts/linear-models_ml/">Linear-Models(ML)</a>
</div>



<div  class="autonumber" >

  <div class="single-intro-container">

    

    <h1 class="single-title">Linear-Models(ML)</h1>
    
    <p class="single-summary">선형 모델(머신러닝)</p>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2025-04-02T18:42:01&#43;09:00">April 2, 2025</time>
      

      
      &nbsp; · &nbsp;
      5 min read
      
    </p>

  </div>

  

  
  

  <div class="single-tags">
    
    <span>
      <a href="https://gobad820.github.io/tags/ml/">#ML</a>
    </span>
    
    
    <span>
      <a href="https://gobad820.github.io/tags/linearmodel/">#LinearModel</a>
    </span>
    
    
  </div>

  
  

  

  
  <aside class="toc">
    <p><strong>Table of contents</strong></p>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#linear-models">Linear Models</a>
      <ul>
        <li><a href="#uni-variate-linear-regression">Uni variate Linear Regression</a></li>
        <li><a href="#multivariate-linear-regression">Multivariate Linear Regression</a></li>
        <li><a href="#linear-classifier-with-a-hard-threshold">Linear classifier with a hard threshold</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </aside>
  

  

  <div class="single-content">
    <blockquote>
<p>2025-04-02 오탈자 수정</p>
</blockquote>
<h2 class="heading" id="linear-models">
  Linear Models
  <a href="#linear-models">#</a>
</h2>
<p>선형 모델은 numeric attribute들을 이용하여 수행된다.
Regression, Classification을 수행하는 모델들을 먼저 학습한다.</p>
<ul>
<li>Regression
<ul>
<li>Uni variate case</li>
<li>Multivariate case</li>
</ul>
</li>
<li>Classification
<ul>
<li>Hard Threshold</li>
<li>Soft threshold: logistic regression</li>
</ul>
</li>
</ul>
<h3 class="heading" id="uni-variate-linear-regression">
  Uni variate Linear Regression
  <a href="#uni-variate-linear-regression">#</a>
</h3>
<p>데이터에 가장 fit한 $h_w$는 $h_w(x) = w_1x + w_0$을 통하여 찾을 수 있다.
이 때 weights들의 값을 찾기 위해서는 $L_2$ loss 값을 최소화 시켜야 한다.
$$
Loss(h_w) = \sum_{j=1}^N L_2(y_j,h_w(x_j)) = \sum_{j=1}^N(y_j-h_w(x_j))^2= \sum_{j1}^N(y_j-(w_1x_j + w_0))^2
$$</p>
<p>$\sum_{j=1}^N(y_j-(w_1x_j + w_0))^2$를 최소화 하기 위해서 $w_0$와 $w_1$에 대해 모두 편미분을 해야 한다.
$$
\begin{align*}
&amp;\frac{\partial}{\partial w_0}\sum_{j=1}^N (y_j-(w_1x_j + w_0)) = -2\sum_{j=1}^N(y_j-(w_1x_j + w_0)) = 0\newline
&amp;\rightarrow w_0 = (\sum y_j-w_1(\sum x_j))\newline\newline
&amp;\frac{\partial}{\partial w_1}\sum_{j=1}^N (y_j-(w_1x_j + w_0)) = -2\sum_{j=1}^N(y_j-(w_1x_j + w_0))x_j = 0\newline
&amp;\rightarrow w_1 = \frac{N(\sum x_j,y_j)-(\sum x_j)(\sum y_j)}{N(\sum x_j^2)-(\sum x_j)^2}\newline
\end{align*}
$$</p>
<p>$L_2$ loss function은 weight space에서 Convex함으로 local minima는 고려하지 않아도 된다.</p>
<p>linear model에 대해서 gradient descent를 적용하면 $w_i \leftarrow w_i - \alpha \frac{\partial}{\partial w_i} Loss(w)$를 만족한다.
여기서 $\alpha$는 learning rate이며 상수이다.
$\alpha$값이 너무 작으면 정확하나 시간이 오래 걸린다는 단점이 있고
$\alpha$값이 너무 크면 비교적 빠르나 정확도가 떨어지게 된다.
따라서 leraning rate는 decay over time을 적용해 시간이 지날수록 값을 축소시킨다.</p>
<p>uni variate linear regression에는 아래와 같은 update rule이 사용된다.</p>
<ul>
<li>Batch gradient descent</li>
<li>Stocahstic gradint descent</li>
</ul>
<h4 class="heading" id="batch-gradietn-descent">
  Batch gradietn descent
  <a href="#batch-gradietn-descent">#</a>
</h4>
<ul>
<li>step마다 모든 트레이닝 데이터 사이클을 반복한다.</li>
<li>$\alpha$값이 작기 때문에 convergence(수렴)을 보장한다.</li>
<li>속도가 느리다.</li>
</ul>
<h4 class="heading" id="stochastic-gradient-descent">
  Stochastic gradient descent
  <a href="#stochastic-gradient-descent">#</a>
</h4>
<ul>
<li>single sample 또는 small set of samples에 대해서 step을 반복한다.</li>
<li>convergence를 보장하지 않는다.</li>
<li>속도가 빠르다.</li>
</ul>
<p>다시 돌아와서 $\frac{\partial}{\partial w_0}Loss(w)$를 이용하여 <strong>percentron learning rate</strong>를 표현할 수 있다.
$$
w_0\leftarrow w_0 + \alpha (y-h_w(x))\newline
w_1\leftarrow w_1 + \alpha (y-h_w(x))\times x
$$</p>
<p>batch learning rule을 통해 모든 N개의 traing sample을 이용할 수 있다.
$$
w_0\leftarrow w_0 + \alpha\sum_j^N (y_j-h_w(x_j))\newline
w_1\leftarrow w_1 + \alpha\sum_j^N (y_j-h_w(x_j))\times x_j
$$</p>
<h3 class="heading" id="multivariate-linear-regression">
  Multivariate Linear Regression
  <a href="#multivariate-linear-regression">#</a>
</h3>
<p>위의 Uni variate와 다르게 각 $x_j$가 $n+1$의 vector인 input attribute values라면 multivariate이다.
$$
\begin{align*}
&amp;h_w(x_j) = w_0x_{j,0} + w_1x_{j,1} + \cdots + w_nx_{j,n} \newline
&amp;= \sum_i w_ix_{j,i} = w\cdot x_j = w^Tx_j\newline
&amp;where\ x_{j,0}=1\ \text{is a dummy input attribute(bias)}
\end{align*}
$$</p>
<p>best weight vector($w^*$) 는 loss를 minimize 시킨다.
$$
w^* = \argmin_w \sum_j L_2(y_j,w^Tx_j)
$$</p>
<p>Gradient descent로 weight를 구할 수 있다.
$$
w_i \leftarrow w_i +\alpha \sum_j x_{j,i} (y_j-h_w(x_j))
$$</p>
<p>Uni variate linear regression과 마찬가지로 $w^*$는 행렬의 역산으로도 바로 구할 수 있다.
$$
w^* = (X^TX)^{-1}X^Ty
$$</p>
<p>관계가 없는 attribute들로 인한 오버피팅을 피하기 위해서는 <strong>regularization</strong>이 필요하다.</p>
<p>$$
\begin{align*}
&amp;Cost(h) = Loss(h) + \lambda Complexity(h)\newline
&amp;\hat{h}^* = \argmin_{h\in \mathcal{H}} Cost(h)
\end{align*}
$$</p>
<p>$\lambda$는 모델의 fit와 complexity 사이의 trade off를 제어한다.
최적의 $\lambda$를 찾기 위해서는 <strong>cross-validation</strong>이 필요하다.
uni variate에서는 오버피팅을 상관하지 않아도 되지만 multivariate에서는 <strong>regulization</strong>을 이를 통해 방지해야 한다.</p>
<p>$Complexity(h_w) = R_q(w) = \sum_i|w_i|^q$이며 각각 $R_1$은 $L1\ norm$ $R_2$는 $L2\ norm$을 나타낸다.
$R_1$은 weihts를 0으로 만들기 위해 sparse model을 만들게 되는 경향이 있다.
이를 <strong>feature selection</strong>효과라고 한다.
그렇기 떄문에 미분 불가능한 지점이 있고 이는 선형 해석이 안된다.
따라서 역행렬을 구하거나 유도해서 풀 수 없으므로 닫힌 해가 없고 직접 풀 수 없다.</p>
<p>$R_2$는 제곱을 하는 연산으로 인해 원형을 형성하며 모든 가중치를 조금씩 줄이는 방식이다.
닫힌 해가 존재하며 수학적으로 직접 풀 수 있다.
또한 <strong>feature selection</strong>은 이루어지지 않으므로 overfitting 방지에 효과적이다.</p>
<p><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>











<figure class="">

    <div>
        <img loading="lazy" alt="L1 L2 regularization image" src="/images/L1-L2-norm.png" >
    </div>

    
</figure>
</p>
<h4 class="heading" id="l1-norm">
  L1 norm
  <a href="#l1-norm">#</a>
</h4>
<p>$$
||x||_1 = \sum i = 1^n|x_i|
$$</p>
<p>$L1-norm$은 각 원소의 절댓값의 합으로 표현된다.
따라서 $||x||_1 = 1$일 때 $(1,0)$ 또는 $(0,1)$이 되거나 $(\frac{1}{2}, \frac{1}{2})$가 된다.
이는 기하학적으로 좌우/상하 직각으로만 움직일 수 있다.
등거리로 표현하면 다이아몬드 형태가 나온다.</p>
<p>머신 러닝에서 L1 norm은 <strong>regularization</strong>에 사용되어 모델의 복자볻를 줄이고 sparse model을 만드는데 사용된다.</p>
<h4 class="heading" id="l2-norm">
  L2 norm
  <a href="#l2-norm">#</a>
</h4>
<p>$$
||x||_2 =  \sqrt{\sum i = 1^nx_i^n} = \sqrt{x^Tx}
$$</p>
<p>L2 norm은 Euclidean norm이라고도 불리며 벡터의 크기를 나타낸다.
즉 벡터의 내적을 의미한다.
기하학적으로는 $||x||_2 = r$일 2차원에서는 원, 3차원에서는 구를 나타낸다.</p>
<p>해당 이미지에서의 등고선은 Loss funciton의 같은 값을 갖는 점들의 집합을 의미한다.
회색 도형은 각각 L1과 L2에 의해 제한된 공간 즉 제약 조건(<strong>C</strong>)를 의미한다.</p>
<h4 class="heading" id="regularization">
  Regularization
  <a href="#regularization">#</a>
</h4>
<p>$$
Cost(h) = Loss(h) + \lambda(Complexity(h) -c)
$$</p>
<p>이는 손실을 최소화하되 복잡도는 일정 이하로 제한하라는 의미이다.</p>
<p>이미지에서의 동심원은 Loss(h)를 나타내며 중심에서 멀어질수록 손실이 큼을 나타낸다.
이 때 학습을 통해 중심에 최대한 가까운 점 즉 최소 Loss를 찾고 싶다.</p>
<p>회색 도형부는 정규화 영역으로 $Complexity(h) \le c$로 표현된다.
해당 조건은 어떤 도형 안에 모델 파라미터가 있어야 함을 나타낸다.</p>
<p>trainig을 통해 손실이 가장 낮으면서 제약 조건 안에 있는 점을 찾아야 한다.
즉 동심원으로 이루어진 등고선과 제약 영역의 점정르 찾아야 한다.</p>
<p><strong>L1</strong>의 경우 다이아몬드 영역에서 꼭짓점과 등고선이 서로 만나 접점을 이룬다.
이 때 feature selection으로 인해 $w$는 sparse matrix가 될 가능성이 있다.</p>
<p><strong>L2</strong>의 경우 원형 영역에서는 접점은 feature selection이 이루어지지 않아 <strong>Dense solution</strong>이 추론된다.
즉 $w_i$값은 작을 수 있어도 0은 아니게 된다.</p>
<blockquote>
<p>라그랑주 승수법(Lagrangian)
제약 조건을 손실 함수 안으로 옮겨서 penalty로 처리하는것
따라서 Loss를 최소화하되 Complexity를 c라는 제약 조건 안에 두기 위해
Loss + (complexity -c)로 표현하며 제약 조건을 벗어나면 penalty가 커져 Cost값도 올라간다.</p>
</blockquote>
<p><strong>정리</strong></p>
<ol>
<li>Regulization은 Overfitting을 방지하기 위하여 사용한다.</li>
<li>L1 norm과 L2 norm 2가지를 사용할 수 있다.</li>
<li>$Cost(h) = Loss(h) + \lambda(Complexity-c)$</li>
<li>Cost를 최소화하기 위해서 Loss를 최소화, Complexity는 페널티로 부과, norm 영역 안에 있도록 강제한다.</li>
<li>c는 사용한 norm에 따라 다르며 L1 norm은 다이아몬드 형태, L2 norm은 원형 형태를 가진다.</li>
</ol>
<h3 class="heading" id="linear-classifier-with-a-hard-threshold">
  Linear classifier with a hard threshold
  <a href="#linear-classifier-with-a-hard-threshold">#</a>
</h3>
<p>$\text{Linear descision boundary }= w\cdot x = 0\ (x_0 = 1\ \text{is a dummy input})$</p>
<p>$\text{Classification hypo} = h_w(x) =1\ \text{if}\ w\cdot x\le 0\ \text{and}\ 0\ \text{otherwise}$</p>
<p>즉 $w\cdot x \ge 0$이면 예측값은 1, 반대로 $w\cdot x &lt; 0$이면 예측값은 0이 된다.</p>
<p>$$
h_w(x) = Threshold(w\cdot x)
$$</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Efficient Feature Selection for Prediction of Diabetic Using LASSO - Scientific Figure on ResearchGate. Available from: <a href="https://www.researchgate.net/figure/Estimation-graph-for-LASSO-left-side-and-Ridge-Regression-right-side-Shown-are_fig1_339765709">https://www.researchgate.net/figure/Estimation-graph-for-LASSO-left-side-and-Ridge-Regression-right-side-Shown-are_fig1_339765709</a> [accessed 2 Apr 2025]&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  


  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/posts/linear-model/">
                        Linear Model
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
            <div class="single-pagination-container-next">
                <div class="single-pagination-text">
                    <a href="/posts/back-propagation/">
                        Back Propagation
                    </a>
                </div>
                <div class="single-pagination-text">→</div>
            </div>
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      

    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        and
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
    
    function getTheme() {
        return document.documentElement.classList.contains('dark') ? 'dark' : 'default';
    }

    
    function initMermaid() {
        mermaid.initialize({ theme: getTheme() });
        mermaid.init();
    }

    
    const observer = new MutationObserver(() => {
        initMermaid();
    });

    observer.observe(document.documentElement, { attributes: true, attributeFilter: ['class'] });

    
    document.addEventListener('DOMContentLoaded', () => {
        initMermaid();
    });
</script>
    </footer>

    
    <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>
    

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>